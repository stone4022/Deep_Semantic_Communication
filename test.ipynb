{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from w3lib.html import remove_tags\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from transformers import BartTokenizer\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input-data-dir', default='europarl/en', type=str)\n",
    "parser.add_argument('--output-train-dir', default='europarl/train_data.pkl', type=str)\n",
    "parser.add_argument('--output-test-dir', default='europarl/test_data.pkl', type=str)\n",
    "parser.add_argument('--output-vocab', default='europarl/vocab.json', type=str)\n",
    "\n",
    "SPECIAL_TOKENS = {\n",
    "  '<PAD>': 0,\n",
    "  '<START>': 1,\n",
    "  '<END>': 2,\n",
    "  '<UNK>': 3,\n",
    "}\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    # normalize unicode characters\n",
    "    s = unicode_to_ascii(s)\n",
    "    # remove the XML-tags\n",
    "    s = remove_tags(s)\n",
    "    # add white space before !.?\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    # change to lower letter\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "def cutted_data(cleaned, MIN_LENGTH=4, MAX_LENGTH=30):\n",
    "    cutted_lines = list()\n",
    "    for line in cleaned:\n",
    "        length = len(line.split())\n",
    "        if length > MIN_LENGTH and length < MAX_LENGTH:\n",
    "            line = [word for word in line.split()]\n",
    "            cutted_lines.append(' '.join(line))\n",
    "    return cutted_lines\n",
    "\n",
    "def save_clean_sentences(sentence, save_path):\n",
    "    pickle.dump(sentence, open(save_path, 'wb'))\n",
    "    print('Saved: %s' % save_path)\n",
    "\n",
    "def process(text_path):\n",
    "    fop = open(text_path, 'r', encoding='utf8')\n",
    "    raw_data = fop.read()\n",
    "    sentences = raw_data.strip().split('\\n')\n",
    "    raw_data_input = [normalize_string(data) for data in sentences]\n",
    "    raw_data_input = cutted_data(raw_data_input)\n",
    "    fop.close()\n",
    "\n",
    "    return raw_data_input\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    data_dir = './data/'\n",
    "\n",
    "    args.input_data_dir = data_dir + args.input_data_dir\n",
    "    args.output_train_dir = data_dir + args.output_train_dir\n",
    "    args.output_test_dir = data_dir + args.output_test_dir\n",
    "    args.output_vocab = data_dir + args.output_vocab\n",
    "\n",
    "    print(args.input_data_dir)\n",
    "    sentences = []\n",
    "    print('Preprocess Raw Text')\n",
    "    for fn in tqdm(os.listdir(args.input_data_dir)):\n",
    "        if not fn.endswith('.txt'): continue\n",
    "        process_sentences = process(os.path.join(args.input_data_dir, fn))\n",
    "        sentences += process_sentences\n",
    "\n",
    "    # remove the same sentences\n",
    "    a = {}\n",
    "    for set in sentences:\n",
    "        if set not in a:\n",
    "            a[set] = 0\n",
    "        a[set] += 1\n",
    "    sentences = list(a.keys())\n",
    "    print('Number of sentences: {}'.format(len(sentences)))\n",
    "    \n",
    "    # use BartTokenizer\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "    print('Start encoding txt')\n",
    "    results = []\n",
    "    for seq in tqdm(sentences):\n",
    "        tokens = tokenizer(seq)\n",
    "        results.append(tokens)\n",
    "\n",
    "    print(results[1],results[2])\n",
    "\n",
    "    print('Writing Data')\n",
    "    train_data = results[: round(len(results) * 0.9)]\n",
    "    test_data = results[round(len(results) * 0.9):]\n",
    "\n",
    "    with open(args.output_train_dir, 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(args.output_test_dir, 'wb') as f:\n",
    "        pickle.dump(test_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "print('Start encoding txt')\n",
    "results = []\n",
    "for seq in tqdm(sentences):\n",
    "    tokens = tokenizer(seq)['input_ids']\n",
    "    results.append(tokens)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9226, 16, 70, 11, 10753, 19, 5, 7797, 14, 52, 33, 460, 14817, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} {'input_ids': [0, 31653, 47, 475, 338, 842, 16993, 118, 939, 5658, 109, 98, 36811, 479, 5329, 24, 16, 1341, 11, 2396, 19, 5, 2452, 42, 790, 34, 460, 5091, 479, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(results[1],results[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Data\n"
     ]
    }
   ],
   "source": [
    "print('Writing Data')\n",
    "train_data = results[: round(len(results) * 0.9)]\n",
    "test_data = results[round(len(results) * 0.9):]\n",
    "\n",
    "with open(args.output_train_dir, 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open(args.output_test_dir, 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer, BartModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BartModel.from_pretrained(\"facebook/bart-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "BartModel(\n",
      "  (shared): Embedding(50265, 768, padding_idx=1)\n",
      "  (encoder): BartEncoder(\n",
      "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0): BartEncoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): BartEncoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): BartEncoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): BartEncoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): BartEncoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): BartEncoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): BartDecoder(\n",
      "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
      "    (layers): ModuleList(\n",
      "      (0): BartDecoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): BartDecoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): BartDecoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): BartDecoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): BartDecoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): BartDecoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")['input_ids']\n",
    "outputs = model(inputs)\n",
    "print(inputs.shape)\n",
    "print(model)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 8, 768])\n",
      "tensor([[[ 2.4118,  2.3732,  1.1981,  ...,  1.8372, -0.1712, -0.7264],\n",
      "         [-1.4809, -0.5842, -3.3371,  ...,  1.1434, -1.9142,  1.5422],\n",
      "         [ 0.8170,  1.5384, -1.3417,  ...,  0.5091, -0.9715,  1.4299],\n",
      "         ...,\n",
      "         [-1.5440,  0.2834, -1.0513,  ...,  0.7554, -0.3832, -0.0514],\n",
      "         [ 1.0442, -0.1567,  2.8073,  ...,  1.2079, -1.3359,  0.0742],\n",
      "         [-0.0903, -0.2080,  0.1134,  ...,  1.1163, -1.0827,  0.3815]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(inputs)\n",
    "print(last_hidden_states.shape)\n",
    "print(last_hidden_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec35eb9b335c7e254c8f905d982b9b8442353d7ca0606de2b8530d7f26352d53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
