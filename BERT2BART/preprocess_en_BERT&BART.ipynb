{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from w3lib.html import remove_tags\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    # normalize unicode characters\n",
    "    s = unicode_to_ascii(s)\n",
    "    # remove the XML-tags\n",
    "    s = remove_tags(s)\n",
    "    # add white space before !.?\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    # change to lower letter\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "def cutted_data(cleaned, MIN_LENGTH=4, MAX_LENGTH=30):\n",
    "    ss = []\n",
    "    for x in cleaned:\n",
    "        if 30>=len(x.split(' '))>=3 :\n",
    "            ss.append(x)\n",
    "    return ss\n",
    "\n",
    "def save_clean_sentences(sentence, save_path):\n",
    "    pickle.dump(sentence, open(save_path, 'wb'))\n",
    "    print('Saved: %s' % save_path)\n",
    "\n",
    "def process(text_path):\n",
    "    fop = open(text_path, 'r', encoding='utf8')\n",
    "    raw_data = fop.read()\n",
    "    sentences = raw_data.strip().split('\\n')\n",
    "    raw_data_input = [normalize_string(data) for data in sentences]\n",
    "    raw_data_input = cutted_data(raw_data_input)\n",
    "    fop.close()\n",
    "\n",
    "    return raw_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess Raw Text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9672/9672 [01:42<00:00, 94.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 119032\n",
      "Start encoding txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119032/119032 [01:04<00:00, 1856.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Data\n",
      "resumption of the session\n",
      "[[101, 24501, 24237, 3508, 1997, 1996, 5219, 102], [0, 1535, 21236, 9, 5, 1852, 2]]\n",
      " the house rose and observed a minute s silence \n",
      "[[101, 1996, 2160, 3123, 1998, 5159, 1037, 3371, 1055, 4223, 102], [0, 5, 790, 1458, 8, 6373, 10, 2289, 579, 7308, 1437, 2]]\n",
      "that is precisely the time when you may if you wish raise this question i .e . on thursday prior to the start of the presentation of the report .\n",
      "[[101, 2008, 2003, 10785, 1996, 2051, 2043, 2017, 2089, 2065, 2017, 4299, 5333, 2023, 3160, 1045, 1012, 1041, 1012, 2006, 9432, 3188, 2000, 1996, 2707, 1997, 1996, 8312, 1997, 1996, 3189, 1012, 102], [0, 6025, 16, 12810, 5, 86, 77, 47, 189, 114, 47, 2813, 1693, 42, 864, 939, 479, 242, 479, 15, 3553, 46806, 2052, 7, 5, 386, 9, 5, 5209, 9, 5, 266, 479, 2]]\n",
      "this is all in accordance with the principles that we have always upheld .\n",
      "[[101, 2023, 2003, 2035, 1999, 10388, 2007, 1996, 6481, 2008, 2057, 2031, 2467, 16813, 1012, 102], [0, 9226, 16, 70, 11, 10753, 19, 5, 7797, 14, 52, 33, 460, 14817, 479, 2]]\n",
      "thank you mr segni i shall do so gladly . indeed it is quite in keeping with the positions this house has always adopted .\n",
      "[[101, 4067, 2017, 2720, 7367, 29076, 1045, 4618, 2079, 2061, 24986, 1012, 5262, 2009, 2003, 3243, 1999, 4363, 2007, 1996, 4460, 2023, 2160, 2038, 2467, 4233, 1012, 102], [0, 31653, 47, 475, 338, 842, 16993, 118, 939, 5658, 109, 98, 36811, 479, 5329, 24, 16, 1341, 11, 2396, 19, 5, 2452, 42, 790, 34, 460, 5091, 479, 2]]\n",
      "yes mrs schroedter i shall be pleased to look into the facts of this case when i have received your letter .\n",
      "[[101, 2748, 3680, 8040, 8093, 29099, 3334, 1045, 4618, 2022, 7537, 2000, 2298, 2046, 1996, 8866, 1997, 2023, 2553, 2043, 1045, 2031, 2363, 2115, 3661, 1012, 102], [0, 10932, 475, 4926, 8447, 1001, 196, 1334, 939, 5658, 28, 4343, 7, 356, 88, 5, 4905, 9, 42, 403, 77, 939, 33, 829, 110, 1601, 479, 2]]\n",
      "in any event this question is not presently included among the requests for topical and urgent debate on thursday .\n",
      "[[101, 1999, 2151, 2724, 2023, 3160, 2003, 2025, 12825, 2443, 2426, 1996, 11186, 2005, 25665, 1998, 13661, 5981, 2006, 9432, 1012, 102], [0, 179, 143, 515, 42, 864, 16, 45, 10375, 1165, 566, 5, 5034, 13, 33469, 8, 9047, 2625, 15, 3553, 46806, 479, 2]]\n",
      "relating to wednesday \n",
      "[[101, 8800, 2000, 9317, 102], [0, 5982, 1295, 7, 18862, 46836, 1437, 2]]\n",
      " applause from the pse group \n",
      "[[101, 20737, 2013, 1996, 8827, 2063, 2177, 102], [0, 17288, 31, 5, 35504, 333, 1437, 2]]\n",
      " applause from the ppe de group \n",
      "[[101, 20737, 2013, 1996, 4903, 2063, 2139, 2177, 102], [0, 17288, 31, 5, 181, 2379, 263, 333, 1437, 2]]\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input-data-dir', default='europarl/en', type=str)\n",
    "parser.add_argument('--output-train-dir', default='BERT&BART/train_data.pkl', type=str)\n",
    "parser.add_argument('--output-test-dir', default='BERT&BART/test_data.pkl', type=str)\n",
    "\n",
    "    \n",
    "args = parser.parse_args(args=[])\n",
    "data_dir = '../data/'\n",
    "\n",
    "args.input_data_dir = data_dir + args.input_data_dir\n",
    "args.output_train_dir = data_dir + args.output_train_dir\n",
    "args.output_test_dir = data_dir + args.output_test_dir\n",
    "\n",
    "sentences = []\n",
    "print('Preprocess Raw Text')\n",
    "for fn in tqdm(os.listdir(args.input_data_dir)):\n",
    "    if not fn.endswith('.txt'): continue\n",
    "    process_sentences = process(os.path.join(args.input_data_dir, fn))\n",
    "    sentences += process_sentences\n",
    "\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "\n",
    "# Tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "print('Start encoding txt')\n",
    "results = []\n",
    "for seq in tqdm(sentences):\n",
    "    bert_tokens = bert_tokenizer(seq)['input_ids']\n",
    "    bart_tokens = bart_tokenizer(seq)['input_ids']\n",
    "    results.append([bert_tokens, bart_tokens])\n",
    "\n",
    "print('Writing Data')\n",
    "train_data = results[: round(len(results) * 0.9)]\n",
    "test_data = results[round(len(results) * 0.9):]\n",
    "\n",
    "with open(args.output_train_dir, 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open(args.output_test_dir, 'wb') as f:\n",
    "    pickle.dump(test_data, f)\n",
    "\n",
    "# spot check\n",
    "for i in range(10):\n",
    "    print(sentences[i])\n",
    "    print(results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[101, 101,   0, 100, 102, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = bert_tokenizer(\"[CLS][PAD][UNK][SEP]\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119032\n",
      "max token len of bert:  65\n",
      "max token len of bart:  69\n"
     ]
    }
   ],
   "source": [
    "print(len(results))\n",
    "max_len_bert = max(len(results[i][0]) for i in range(len(results)))\n",
    "max_len_bart = max(len(results[i][1]) for i in range(len(results)))\n",
    "\n",
    "print('max token len of bert: ', max_len_bert)\n",
    "print('max token len of bart: ', max_len_bart)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec35eb9b335c7e254c8f905d982b9b8442353d7ca0606de2b8530d7f26352d53"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
