{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "from w3lib.html import remove_tags\n",
    "import pickle\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def normalize_string(s):\n",
    "    # normalize unicode characters\n",
    "    s = unicode_to_ascii(s)\n",
    "    # remove the XML-tags\n",
    "    s = remove_tags(s)\n",
    "    # add white space before !.?\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    # change to lower letter\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "def cutted_data(cleaned, MIN_LENGTH=4, MAX_LENGTH=30):\n",
    "    ss = []\n",
    "    for x in cleaned:\n",
    "        if 30>=len(x.split(' '))>=3 :\n",
    "            ss.append(x)\n",
    "    return ss\n",
    "\n",
    "def save_clean_sentences(sentence, save_path):\n",
    "    pickle.dump(sentence, open(save_path, 'wb'))\n",
    "    print('Saved: %s' % save_path)\n",
    "\n",
    "def process(text_path):\n",
    "    fop = open(text_path, 'r', encoding='utf8')\n",
    "    raw_data = fop.read()\n",
    "    sentences = raw_data.strip().split('\\n')\n",
    "    raw_data_input = [normalize_string(data) for data in sentences]\n",
    "    raw_data_input = cutted_data(raw_data_input)\n",
    "    fop.close()\n",
    "\n",
    "    return raw_data_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocess Raw Text\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系統找不到指定的路徑。: '../data/europarl/en'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aries\\Downloads\\semantic\\Deep_Semantic_Communication\\BERT2BART\\preprocess_en_BERT&BART.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aries/Downloads/semantic/Deep_Semantic_Communication/BERT2BART/preprocess_en_BERT%26BART.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m sentences \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aries/Downloads/semantic/Deep_Semantic_Communication/BERT2BART/preprocess_en_BERT%26BART.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPreprocess Raw Text\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aries/Downloads/semantic/Deep_Semantic_Communication/BERT2BART/preprocess_en_BERT%26BART.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m fn \u001b[39min\u001b[39;00m tqdm(os\u001b[39m.\u001b[39;49mlistdir(args\u001b[39m.\u001b[39;49minput_data_dir)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aries/Downloads/semantic/Deep_Semantic_Communication/BERT2BART/preprocess_en_BERT%26BART.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fn\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aries/Downloads/semantic/Deep_Semantic_Communication/BERT2BART/preprocess_en_BERT%26BART.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     process_sentences \u001b[39m=\u001b[39m process(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(args\u001b[39m.\u001b[39minput_data_dir, fn))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系統找不到指定的路徑。: '../data/europarl/en'"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--input-data-dir', default='europarl/en', type=str)\n",
    "parser.add_argument('--output-train-dir', default='BERT&BART/train_data.pkl', type=str)\n",
    "parser.add_argument('--output-test-dir', default='BERT&BART/test_data.pkl', type=str)\n",
    "\n",
    "    \n",
    "args = parser.parse_args(args=[])\n",
    "data_dir = '../data/'\n",
    "\n",
    "args.input_data_dir = data_dir + args.input_data_dir\n",
    "args.output_train_dir = data_dir + args.output_train_dir\n",
    "args.output_test_dir = data_dir + args.output_test_dir\n",
    "\n",
    "sentences = []\n",
    "print('Preprocess Raw Text')\n",
    "for fn in tqdm(os.listdir(args.input_data_dir)):\n",
    "    if not fn.endswith('.txt'): continue\n",
    "    process_sentences = process(os.path.join(args.input_data_dir, fn))\n",
    "    sentences += process_sentences\n",
    "\n",
    "print('Number of sentences: {}'.format(len(sentences)))\n",
    "\n",
    "# Tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "print('Start encoding txt')\n",
    "results = []\n",
    "for seq in tqdm(sentences):\n",
    "    bert_tokens = bert_tokenizer(seq)['input_ids']\n",
    "    bart_tokens = bart_tokenizer(seq)['input_ids']\n",
    "\n",
    "    results.append([bert_tokens, bart_tokens])\n",
    "\n",
    "print('Writing Data')\n",
    "train_data = results[: round(len(results) * 0.9)]\n",
    "test_data = results[round(len(results) * 0.9):]\n",
    "\n",
    "with open(args.output_train_dir, 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open(args.output_test_dir, 'wb') as f:\n",
    "    pickle.dump(test_data, f)\n",
    "\n",
    "# spot check\n",
    "for i in range(10):\n",
    "    print(sentences[i])\n",
    "    print(results[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[101, 101,   0, 100, 102, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"[CLS][PAD][UNK][SEP]\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119032\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "print(len(results))\n",
    "max_len = 0\n",
    "for i in range(len(results)):\n",
    "    tmp = len(results[i])\n",
    "    if tmp > max_len:\n",
    "        max_len = tmp\n",
    "print(max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d3d303814ca5ff286526fa90e0ab70cd661e2119bac7791e3f542b984082dd37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
